# Model Configuration
model_name: "mtgv/MobileLLaMA-1.4B-Chat"
new_model_name: "sbicard-slm-1.4b"

# Dataset Configuration
dataset_path: "data_processing/processed_data/sbicard_instructions.jsonl" # Path on EC2 after upload
max_seq_length: 512

# QLoRA Configuration
load_in_4bit: true
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
target_modules:
  - "q_proj"
  - "k_proj"
  - "v_proj"
  - "o_proj"
  - "gate_proj"
  - "up_proj"
  - "down_proj"

# Training Arguments
output_dir: "./results"
num_train_epochs: 3
per_device_train_batch_size: 4
gradient_accumulation_steps: 4 # Effective batch size = 16
learning_rate: 2.0e-4
weight_decay: 0.001
fp16: true
bf16: false # Use bf16 if Ampere GPU (A10G) supports it, otherwise fp16
max_grad_norm: 0.3
warmup_ratio: 0.03
group_by_length: true
lr_scheduler_type: "cosine"
logging_steps: 10
save_steps: 100
eval_steps: 100

# Push to Hub
push_to_hub: false
hub_model_id: "your-username/sbicard-slm-1.4b"
